{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Gemma 3 1B - Optimized for Small Dataset (3,855 samples)\n",
    "\n",
    "**Model**: Google Gemma 3 1B-IT  \n",
    "**Dataset**: UNSIQ - 771 unique questions √ó 5 variations = 3,855 samples  \n",
    "**Optimization**: Research-based configuration for LIMITED DOMAIN-SPECIFIC DATA\n",
    "\n",
    "## üéØ Small Dataset Challenges:\n",
    "- ‚ö†Ô∏è **Overfitting Risk**: Model memorizes instead of learning patterns\n",
    "- ‚ö†Ô∏è **Limited Context**: Less exposure to diverse scenarios\n",
    "- ‚ö†Ô∏è **Generalization Issues**: May not perform well on unseen data\n",
    "\n",
    "## ‚úÖ Our Solutions (Research-Based 2025):\n",
    "1. **Lower LoRA Rank** (32 vs 64): Reduce capacity to prevent memorization\n",
    "2. **Higher Dropout** (0.1 vs 0.05): Stronger regularization\n",
    "3. **Higher Weight Decay** (0.05 vs 0.01): L2 regularization\n",
    "4. **Fewer Epochs** (2 vs 3): Prevent overfitting from repeated exposure\n",
    "5. **Lower Learning Rate** (5e-5 vs 2e-4): Conservative, stable training\n",
    "6. **Early Stopping** (patience=5): Auto-stop when validation stops improving\n",
    "7. **Frequent Eval** (every 25 steps): Close monitoring for overfitting\n",
    "\n",
    "## üìö Research Sources:\n",
    "- [Fine-Tuning LLMs on Small Datasets](https://www.sapien.io/blog/strategies-for-fine-tuning-llms-on-small-datasets)\n",
    "- [Unveiling the Secret Recipe: Small LLMs](https://arxiv.org/html/2412.13337v1)\n",
    "- [LoRA Hyperparameters Guide](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide)\n",
    "- [Practical LoRA Tips](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U torch>=2.4.0 transformers>=4.50.0 accelerate bitsandbytes peft datasets trl tensorboard sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {__import__('transformers').__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Small Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"qlora_config_gemma3_1b_SMALL_DATASET.json\"\n",
    "\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SMALL DATASET OPTIMIZED CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "for key, value in config['dataset_stats'].items():\n",
    "    print(f\"  ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  LoRA Configuration (Overfitting Prevention):\")\n",
    "print(f\"  ‚Ä¢ Rank (r): {config['qlora_config']['r']} ‚Üê Lower to prevent memorization\")\n",
    "print(f\"  ‚Ä¢ Alpha: {config['qlora_config']['lora_alpha']}\")\n",
    "print(f\"  ‚Ä¢ Dropout: {config['qlora_config']['lora_dropout']} ‚Üê Higher for regularization\")\n",
    "\n",
    "print(f\"\\nüéì Training Configuration (Small Data Optimized):\")\n",
    "print(f\"  ‚Ä¢ Learning Rate: {config['training_args']['learning_rate']} ‚Üê Conservative\")\n",
    "print(f\"  ‚Ä¢ Epochs: {config['training_args']['num_train_epochs']} ‚Üê Limited to prevent overfitting\")\n",
    "print(f\"  ‚Ä¢ Weight Decay: {config['training_args']['weight_decay']} ‚Üê Strong regularization\")\n",
    "print(f\"  ‚Ä¢ Early Stopping Patience: {config['training_args']['early_stopping_patience']}\")\n",
    "print(f\"  ‚Ä¢ Eval Steps: {config['training_args']['eval_steps']} ‚Üê Frequent monitoring\")\n",
    "\n",
    "print(f\"\\nüõ°Ô∏è  Overfitting Prevention Techniques Applied:\")\n",
    "for i, technique in enumerate(config['overfitting_prevention']['techniques_applied'], 1):\n",
    "    print(f\"  {i}. {technique}\")\n",
    "\n",
    "print(f\"\\nüìö Research-Based Configuration:\")\n",
    "for finding in config['research_based_on']['key_findings']:\n",
    "    print(f\"  ‚úì {finding}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model & Tokenizer with QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config['model_config']['model_name']\n",
    "\n",
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"Loading {model_name}...\\n\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_cache=False,\n",
    "    attn_implementation=\"eager\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.padding_side = 'right'\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"‚úì Model loaded: {model.num_parameters():,} parameters\")\n",
    "print(f\"‚úì Tokenizer vocab: {len(tokenizer):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply LoRA - Optimized for Small Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing model for QLoRA...\")\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "# LoRA config optimized for small dataset\n",
    "lora_config = LoraConfig(\n",
    "    r=config['qlora_config']['r'],  # 32 - balanced for small data\n",
    "    lora_alpha=config['qlora_config']['lora_alpha'],  # 64\n",
    "    lora_dropout=config['qlora_config']['lora_dropout'],  # 0.1 - higher dropout\n",
    "    bias=config['qlora_config']['bias'],\n",
    "    task_type=config['qlora_config']['task_type'],\n",
    "    target_modules=config['qlora_config']['target_modules'],\n",
    "    modules_to_save=config['qlora_config'].get('modules_to_save'),\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"\\n‚úì LoRA Applied (Small Dataset Optimized):\")\n",
    "print(f\"  Trainable: {trainable:,} ({100*trainable/total:.4f}%)\")\n",
    "print(f\"  Total: {total:,}\")\n",
    "print(f\"\\n  Lower rank (32) reduces overfitting risk on {config['dataset_stats']['total_samples']} samples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load UNSIQ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading datasets...\\n\")\n",
    "\n",
    "train_dataset = load_dataset('json', data_files=config['dataset_config']['train_file'], split='train')\n",
    "eval_dataset = load_dataset('json', data_files=config['dataset_config']['eval_file'], split='train')\n",
    "\n",
    "print(f\"‚úì Train: {len(train_dataset):,} samples\")\n",
    "print(f\"‚úì Eval: {len(eval_dataset):,} samples ({len(eval_dataset)/(len(train_dataset)+len(eval_dataset))*100:.1f}%)\")\n",
    "print(f\"‚úì Total: {len(train_dataset) + len(eval_dataset):,} samples\")\n",
    "print(f\"\\nüí° 15% validation split is ideal for monitoring overfitting on small datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Format with Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_template(example):\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example['messages'],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {'text': text}\n",
    "\n",
    "train_dataset = train_dataset.map(format_chat_template, desc=\"Formatting train\")\n",
    "eval_dataset = eval_dataset.map(format_chat_template, desc=\"Formatting eval\")\n",
    "\n",
    "print(\"\\n‚úì Datasets formatted with Gemma 3 chat template\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Arguments with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = config['training_args']\n",
    "os.makedirs(args['output_dir'], exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # Output\n",
    "    output_dir=args['output_dir'],\n",
    "    overwrite_output_dir=args['overwrite_output_dir'],\n",
    "    \n",
    "    # Training - Optimized for small data\n",
    "    num_train_epochs=args['num_train_epochs'],  # 2 epochs\n",
    "    per_device_train_batch_size=args['per_device_train_batch_size'],\n",
    "    per_device_eval_batch_size=args['per_device_eval_batch_size'],\n",
    "    gradient_accumulation_steps=args['gradient_accumulation_steps'],\n",
    "    gradient_checkpointing=args['gradient_checkpointing'],\n",
    "    gradient_checkpointing_kwargs=args.get('gradient_checkpointing_kwargs', {}),\n",
    "    \n",
    "    # Optimization - Conservative for small data\n",
    "    optim=args['optim'],\n",
    "    learning_rate=args['learning_rate'],  # 5e-5 - lower\n",
    "    weight_decay=args['weight_decay'],  # 0.05 - higher regularization\n",
    "    max_grad_norm=args['max_grad_norm'],\n",
    "    \n",
    "    # Scheduler\n",
    "    lr_scheduler_type=args['lr_scheduler_type'],\n",
    "    warmup_ratio=args['warmup_ratio'],\n",
    "    warmup_steps=args.get('warmup_steps', 0),\n",
    "    \n",
    "    # Evaluation - Frequent for monitoring\n",
    "    eval_strategy=args['eval_strategy'],\n",
    "    eval_steps=args['eval_steps'],  # Every 25 steps\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_strategy=args['save_strategy'],\n",
    "    save_steps=args['save_steps'],\n",
    "    save_total_limit=args['save_total_limit'],\n",
    "    load_best_model_at_end=args['load_best_model_at_end'],\n",
    "    metric_for_best_model=args['metric_for_best_model'],\n",
    "    greater_is_better=args.get('greater_is_better', False),\n",
    "    \n",
    "    # Logging\n",
    "    logging_strategy=args['logging_strategy'],\n",
    "    logging_steps=args['logging_steps'],\n",
    "    logging_first_step=args.get('logging_first_step', True),\n",
    "    report_to=args['report_to'],\n",
    "    \n",
    "    # Precision\n",
    "    bf16=args['bf16'],\n",
    "    bf16_full_eval=args['bf16_full_eval'],\n",
    "    \n",
    "    # Data\n",
    "    dataloader_num_workers=args['dataloader_num_workers'],\n",
    "    dataloader_pin_memory=args.get('dataloader_pin_memory', True),\n",
    "    group_by_length=args['group_by_length'],\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=args.get('seed', 42),\n",
    "    data_seed=args.get('data_seed', 42),\n",
    ")\n",
    "\n",
    "# Calculate stats\n",
    "steps_per_epoch = len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)\n",
    "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING PLAN - SMALL DATASET OPTIMIZED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Total steps: {total_steps}\")\n",
    "print(f\"Evaluations: ~{total_steps // training_args.eval_steps}\")\n",
    "print(f\"Early stopping: Will stop after {args['early_stopping_patience']} evals without improvement\")\n",
    "print(f\"Expected training time: ~30-45 min on A100\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Initialize Trainer with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=config['training_args']['early_stopping_patience'],\n",
    "    early_stopping_threshold=config['training_args'].get('early_stopping_threshold', 0.0)\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field='text',\n",
    "    max_seq_length=config['dataset_config']['max_length'],\n",
    "    packing=config['dataset_config'].get('packing', False),\n",
    "    callbacks=[early_stopping],  # Add early stopping\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer initialized with Early Stopping\")\n",
    "print(f\"  ‚Ä¢ Will monitor: {training_args.metric_for_best_model}\")\n",
    "print(f\"  ‚Ä¢ Patience: {config['training_args']['early_stopping_patience']} evaluations\")\n",
    "print(f\"  ‚Ä¢ Auto-stop if validation loss stops improving!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Start Training with Overfitting Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"STARTING TRAINING - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüîç MONITORING FOR OVERFITTING:\")\n",
    "print(\"  ‚Ä¢ Watch eval_loss vs train_loss\")\n",
    "print(\"  ‚Ä¢ If eval_loss increases while train_loss decreases = OVERFITTING\")\n",
    "print(\"  ‚Ä¢ Early stopping will auto-stop training if detected\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"TRAINING COMPLETED - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFinal Training Loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Runtime: {train_result.metrics['train_runtime']:.2f}s ({train_result.metrics['train_runtime']/60:.1f} min)\")\n",
    "print(f\"Samples/sec: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
    "\n",
    "# Check if early stopping was triggered\n",
    "if 'epoch' in train_result.metrics:\n",
    "    actual_epochs = train_result.metrics['epoch']\n",
    "    planned_epochs = training_args.num_train_epochs\n",
    "    if actual_epochs < planned_epochs:\n",
    "        print(f\"\\n‚ö†Ô∏è  EARLY STOPPING TRIGGERED at epoch {actual_epochs:.2f}/{planned_epochs}\")\n",
    "        print(\"    This means validation loss stopped improving - GOOD!\")\n",
    "    else:\n",
    "        print(f\"\\n‚úì Completed all {planned_epochs} epochs\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Model & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dir = f\"{training_args.output_dir}/final_adapter\"\n",
    "trainer.model.save_pretrained(final_dir)\n",
    "tokenizer.save_pretrained(final_dir)\n",
    "\n",
    "# Save config\n",
    "with open(f\"{training_args.output_dir}/config.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Save metrics\n",
    "with open(f\"{training_args.output_dir}/metrics.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_result.metrics, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Model saved: {final_dir}\")\n",
    "print(f\"‚úì Config saved: {training_args.output_dir}/config.json\")\n",
    "print(f\"‚úì Metrics saved: {training_args.output_dir}/metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFinal evaluation...\\n\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "for k, v in eval_results.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save eval results\n",
    "with open(f\"{training_args.output_dir}/eval_results.json\", 'w') as f:\n",
    "    json.dump(eval_results, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì Evaluation saved: {training_args.output_dir}/eval_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Test Inference - Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question, max_new_tokens=512):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Anda adalah asisten informasi UNSIQ (Universitas Sains Al-Qur'an) yang membantu menjawab pertanyaan tentang biaya kuliah, program studi, dan informasi akademik.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    gen_cfg = config.get('generation_config', {})\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=gen_cfg.get('max_new_tokens', max_new_tokens),\n",
    "            temperature=gen_cfg.get('temperature', 0.7),\n",
    "            top_p=gen_cfg.get('top_p', 0.9),\n",
    "            top_k=gen_cfg.get('top_k', 50),\n",
    "            repetition_penalty=gen_cfg.get('repetition_penalty', 1.1),\n",
    "            do_sample=gen_cfg.get('do_sample', True),\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True).strip()\n",
    "\n",
    "test_questions = [\n",
    "    \"Berapa total biaya kuliah S1 Akuntansi di UNSIQ?\",\n",
    "    \"Apa itu KIP Kuliah dan syaratnya?\",\n",
    "    \"Mengapa semester 1 lebih mahal?\",\n",
    "    \"Program studi apa saja yang ada di UNSIQ?\",\n",
    "    \"Bagaimana sistem pembayaran di UNSIQ?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUALITY CHECK - TESTING FINE-TUNED MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TEST {i}/{len(test_questions)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n‚ùì Q: {q}\")\n",
    "    print(f\"\\nü§ñ A: {generate_response(q)}\")\n",
    "    print(f\"\\n{'-'*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Overfitting Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze training history for overfitting\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERFITTING ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load training history\n",
    "history_file = f\"{training_args.output_dir}/trainer_state.json\"\n",
    "if os.path.exists(history_file):\n",
    "    with open(history_file, 'r') as f:\n",
    "        history = json.load(f)\n",
    "    \n",
    "    log_history = history.get('log_history', [])\n",
    "    \n",
    "    # Extract train and eval losses\n",
    "    train_losses = [(h['step'], h['loss']) for h in log_history if 'loss' in h and 'eval_loss' not in h]\n",
    "    eval_losses = [(h['step'], h['eval_loss']) for h in log_history if 'eval_loss' in h]\n",
    "    \n",
    "    if train_losses and eval_losses:\n",
    "        print(f\"\\nüìä Loss Progression:\")\n",
    "        print(f\"\\nTrain Loss:\")\n",
    "        for step, loss in train_losses[-5:]:\n",
    "            print(f\"  Step {step}: {loss:.4f}\")\n",
    "        \n",
    "        print(f\"\\nEval Loss:\")\n",
    "        for step, loss in eval_losses[-5:]:\n",
    "            print(f\"  Step {step}: {loss:.4f}\")\n",
    "        \n",
    "        # Check for overfitting\n",
    "        if len(eval_losses) >= 2:\n",
    "            last_eval = eval_losses[-1][1]\n",
    "            best_eval = min(e[1] for e in eval_losses)\n",
    "            \n",
    "            print(f\"\\nüéØ Overfitting Check:\")\n",
    "            print(f\"  Best eval loss: {best_eval:.4f}\")\n",
    "            print(f\"  Final eval loss: {last_eval:.4f}\")\n",
    "            \n",
    "            if last_eval > best_eval * 1.05:\n",
    "                print(f\"  ‚ö†Ô∏è  WARNING: Eval loss increased by {((last_eval/best_eval-1)*100):.1f}% - possible overfitting\")\n",
    "            else:\n",
    "                print(f\"  ‚úì GOOD: No significant overfitting detected\")\n",
    "else:\n",
    "    print(\"\\nTrainer history not found. Check TensorBoard for analysis.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. List All Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = sorted(\n",
    "    [d for d in os.listdir(training_args.output_dir) if d.startswith('checkpoint-')],\n",
    "    key=lambda x: int(x.split('-')[-1])\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"SAVED CHECKPOINTS ({len(checkpoints)} total)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, cp in enumerate(checkpoints, 1):\n",
    "    cp_path = os.path.join(training_args.output_dir, cp)\n",
    "    size = sum(\n",
    "        os.path.getsize(os.path.join(root, f))\n",
    "        for root, _, files in os.walk(cp_path)\n",
    "        for f in files\n",
    "    )\n",
    "    step = int(cp.split('-')[-1])\n",
    "    print(f\"{i:2d}. {cp:20s} | Step {step:4d} | {size/1024**2:6.1f} MB\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. TensorBoard Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "print(\"\\nüìä Opening TensorBoard...\")\n",
    "print(\"\\nMonitor these metrics for overfitting:\")\n",
    "print(\"  1. train/loss vs eval/loss - Should decrease together\")\n",
    "print(\"  2. If eval/loss increases while train/loss decreases = OVERFITTING\")\n",
    "print(\"  3. Learning rate schedule\")\n",
    "print(\"  4. Gradient norms\\n\")\n",
    "\n",
    "%tensorboard --logdir {training_args.output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "summary = f\"\"\"\n",
    "{'='*80}\n",
    "TRAINING SUMMARY - SMALL DATASET OPTIMIZED\n",
    "{'='*80}\n",
    "\n",
    "üìä Dataset:\n",
    "  ‚Ä¢ Total samples: {config['dataset_stats']['total_samples']}\n",
    "  ‚Ä¢ Unique questions: {config['dataset_stats']['unique_questions']}\n",
    "  ‚Ä¢ Train: {len(train_dataset)} | Eval: {len(eval_dataset)}\n",
    "  ‚Ä¢ Type: Small domain-specific dataset\n",
    "\n",
    "üîß Configuration (Overfitting Prevention):\n",
    "  ‚Ä¢ LoRA Rank: {config['qlora_config']['r']} (reduced to prevent memorization)\n",
    "  ‚Ä¢ Dropout: {config['qlora_config']['lora_dropout']} (higher for regularization)\n",
    "  ‚Ä¢ Weight Decay: {config['training_args']['weight_decay']} (strong L2 reg)\n",
    "  ‚Ä¢ Learning Rate: {config['training_args']['learning_rate']} (conservative)\n",
    "  ‚Ä¢ Epochs: {config['training_args']['num_train_epochs']} (limited)\n",
    "  ‚Ä¢ Early Stopping: Patience {config['training_args']['early_stopping_patience']}\n",
    "\n",
    "üìà Results:\n",
    "  ‚Ä¢ Final train loss: {train_result.training_loss:.4f}\n",
    "  ‚Ä¢ Final eval loss: {eval_results.get('eval_loss', 'N/A')}\n",
    "  ‚Ä¢ Training time: {train_result.metrics['train_runtime']/60:.1f} minutes\n",
    "  ‚Ä¢ Trainable params: {trainable:,} ({100*trainable/total:.4f}%)\n",
    "\n",
    "‚úÖ Overfitting Prevention Applied:\n",
    "{chr(10).join('  ' + str(i+1) + '. ' + t for i, t in enumerate(config['overfitting_prevention']['techniques_applied']))}\n",
    "\n",
    "üìÅ Output: {training_args.output_dir}/\n",
    "  ‚Ä¢ final_adapter/ - Best model based on eval_loss\n",
    "  ‚Ä¢ checkpoint-*/ - All training checkpoints\n",
    "  ‚Ä¢ metrics.json - Training metrics\n",
    "  ‚Ä¢ eval_results.json - Evaluation results\n",
    "\n",
    "üéì Research-Based:\n",
    "{chr(10).join('  ‚Ä¢ ' + p for p in config['research_based_on']['papers'])}\n",
    "\n",
    "{'='*80}\n",
    "‚úÖ TRAINING COMPLETE - MODEL OPTIMIZED FOR SMALL DATASET\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary\n",
    "with open(f\"{training_args.output_dir}/SUMMARY.txt\", 'w', encoding='utf-8') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(f\"\\n‚úì Summary saved: {training_args.output_dir}/SUMMARY.txt\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
