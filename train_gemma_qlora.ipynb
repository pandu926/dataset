{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Gemma 2B dengan QLoRA - UNSIQ Dataset\n",
    "\n",
    "Notebook ini untuk fine-tuning model Gemma menggunakan QLoRA dengan konfigurasi aggressive untuk A100.\n",
    "\n",
    "**Fitur:**\n",
    "- Menyimpan SEMUA checkpoint (tidak ada batasan)\n",
    "- Menggunakan dataset UNSIQ yang sudah diformat\n",
    "- Konfigurasi aggressive untuk training lebih cepat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U torch>=2.4.0 transformers>=4.51.3 accelerate bitsandbytes peft datasets trl tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config dari file\n",
    "config_path = r\"C:\\Users\\pandu\\Downloads\\qlora_config_A100_AGGRESSIVE.json\"\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Update config untuk save ALL checkpoints\n",
    "config['training_args']['save_total_limit'] = None  # Simpan semua checkpoint\n",
    "config['training_args']['output_dir'] = './outputs/gemma-unsiq-aggressive'\n",
    "\n",
    "# Update dataset paths\n",
    "config['dataset_config']['train_file'] = 'data/train.jsonl'\n",
    "config['dataset_config']['eval_file'] = 'data/eval.jsonl'\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup Model & Tokenizer dengan QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config['model_config']['model_name']\n",
    "\n",
    "# Quantization config untuk 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=config['quantization_config']['load_in_4bit'],\n",
    "    bnb_4bit_compute_dtype=getattr(torch, config['quantization_config']['bnb_4bit_compute_dtype']),\n",
    "    bnb_4bit_quant_type=config['quantization_config']['bnb_4bit_quant_type'],\n",
    "    bnb_4bit_use_double_quant=config['quantization_config']['bnb_4bit_use_double_quant'],\n",
    ")\n",
    "\n",
    "# Load model\n",
    "print(f\"Loading model: {model_name}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=config['model_config']['trust_remote_code'],\n",
    "    torch_dtype=getattr(torch, config['model_config']['torch_dtype']),\n",
    "    use_cache=config['model_config']['use_cache'],\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "# Set pad token jika belum ada\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Model loaded. Total parameters: {model.num_parameters():,}\")\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=config['qlora_config']['r'],\n",
    "    lora_alpha=config['qlora_config']['lora_alpha'],\n",
    "    lora_dropout=config['qlora_config']['lora_dropout'],\n",
    "    bias=config['qlora_config']['bias'],\n",
    "    task_type=config['qlora_config']['task_type'],\n",
    "    target_modules=config['qlora_config']['target_modules'],\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_percent = 100 * trainable_params / all_params\n",
    "\n",
    "print(f\"Trainable params: {trainable_params:,} || All params: {all_params:,} || Trainable%: {trainable_percent:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_dataset = load_dataset('json', data_files=config['dataset_config']['train_file'], split='train')\n",
    "eval_dataset = load_dataset('json', data_files=config['dataset_config']['eval_file'], split='train')\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Eval dataset: {len(eval_dataset)} samples\")\n",
    "\n",
    "# Lihat sample data\n",
    "print(\"\\nSample data:\")\n",
    "print(json.dumps(train_dataset[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Formatting Function untuk Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_template(example):\n",
    "    \"\"\"\n",
    "    Format messages menggunakan tokenizer chat template\n",
    "    \"\"\"\n",
    "    messages = example['messages']\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {'text': text}\n",
    "\n",
    "# Apply formatting\n",
    "train_dataset = train_dataset.map(format_chat_template)\n",
    "eval_dataset = eval_dataset.map(format_chat_template)\n",
    "\n",
    "# Lihat hasil formatting\n",
    "print(\"Formatted example:\")\n",
    "print(train_dataset[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_config = config['training_args']\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=training_args_config['output_dir'],\n",
    "    overwrite_output_dir=training_args_config['overwrite_output_dir'],\n",
    "    num_train_epochs=training_args_config['num_train_epochs'],\n",
    "    per_device_train_batch_size=training_args_config['per_device_train_batch_size'],\n",
    "    per_device_eval_batch_size=training_args_config['per_device_eval_batch_size'],\n",
    "    gradient_accumulation_steps=training_args_config['gradient_accumulation_steps'],\n",
    "    gradient_checkpointing=training_args_config['gradient_checkpointing'],\n",
    "    optim=training_args_config['optim'],\n",
    "    learning_rate=training_args_config['learning_rate'],\n",
    "    weight_decay=training_args_config['weight_decay'],\n",
    "    max_grad_norm=training_args_config['max_grad_norm'],\n",
    "    lr_scheduler_type=training_args_config['lr_scheduler_type'],\n",
    "    warmup_ratio=training_args_config['warmup_ratio'],\n",
    "    eval_strategy=training_args_config['eval_strategy'],\n",
    "    eval_steps=training_args_config['eval_steps'],\n",
    "    save_strategy=training_args_config['save_strategy'],\n",
    "    save_steps=training_args_config['save_steps'],\n",
    "    save_total_limit=training_args_config['save_total_limit'],  # None = save all\n",
    "    load_best_model_at_end=training_args_config['load_best_model_at_end'],\n",
    "    metric_for_best_model=training_args_config['metric_for_best_model'],\n",
    "    logging_strategy=training_args_config['logging_strategy'],\n",
    "    logging_steps=training_args_config['logging_steps'],\n",
    "    report_to=training_args_config['report_to'],\n",
    "    bf16=training_args_config['bf16'],\n",
    "    bf16_full_eval=training_args_config['bf16_full_eval'],\n",
    "    dataloader_num_workers=training_args_config['dataloader_num_workers'],\n",
    "    group_by_length=training_args_config['group_by_length'],\n",
    "    ddp_find_unused_parameters=training_args_config['ddp_find_unused_parameters'],\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured.\")\n",
    "print(f\"Total training steps: {len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")\n",
    "print(f\"Save checkpoint every: {training_args.save_steps} steps\")\n",
    "print(f\"Save total limit: {training_args.save_total_limit} (None = save all)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Setup SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field='text',\n",
    "    max_seq_length=config['dataset_config']['max_length'],\n",
    "    packing=False,  # Disable packing untuk chat format\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting training at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Train\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Training completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Training time: {train_result.metrics['train_runtime']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final adapter\n",
    "final_output_dir = f\"{training_args.output_dir}/final_adapter\"\n",
    "trainer.model.save_pretrained(final_output_dir)\n",
    "tokenizer.save_pretrained(final_output_dir)\n",
    "\n",
    "print(f\"Final adapter saved to: {final_output_dir}\")\n",
    "\n",
    "# Save training metrics\n",
    "metrics_file = f\"{training_args.output_dir}/training_metrics.json\"\n",
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump(train_result.metrics, f, indent=2)\n",
    "    \n",
    "print(f\"Training metrics saved to: {metrics_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Save evaluation results\n",
    "eval_file = f\"{training_args.output_dir}/eval_results.json\"\n",
    "with open(eval_file, 'w') as f:\n",
    "    json.dump(eval_results, f, indent=2)\n",
    "    \n",
    "print(f\"\\nEvaluation results saved to: {eval_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation\n",
    "def generate_response(question):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Anda adalah asisten informasi UNSIQ (Universitas Sains Al-Qur'an) yang membantu menjawab pertanyaan tentang biaya kuliah, program studi, dan informasi akademik.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Format dengan chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract assistant response\n",
    "    if \"model\" in response:\n",
    "        response = response.split(\"model\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test beberapa pertanyaan\n",
    "test_questions = [\n",
    "    \"Berapa biaya kuliah S1 Akuntansi di UNSIQ?\",\n",
    "    \"Apa itu KIP Kuliah?\",\n",
    "    \"Bagaimana cara mendaftar di UNSIQ?\"\n",
    "]\n",
    "\n",
    "print(\"Testing model with sample questions:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\nQ{i}: {question}\")\n",
    "    response = generate_response(question)\n",
    "    print(f\"A{i}: {response}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. List All Saved Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = training_args.output_dir\n",
    "checkpoints = [d for d in os.listdir(output_dir) if d.startswith('checkpoint-')]\n",
    "checkpoints.sort(key=lambda x: int(x.split('-')[-1]))\n",
    "\n",
    "print(f\"Total checkpoints saved: {len(checkpoints)}\")\n",
    "print(\"\\nCheckpoints:\")\n",
    "for cp in checkpoints:\n",
    "    cp_path = os.path.join(output_dir, cp)\n",
    "    size = sum(os.path.getsize(os.path.join(cp_path, f)) for f in os.listdir(cp_path) if os.path.isfile(os.path.join(cp_path, f)))\n",
    "    print(f\"  - {cp} ({size / 1024**2:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. View Training Logs with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {training_args.output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Merge LoRA Adapters (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA weights dengan base model untuk deployment\n",
    "# WARNING: Ini akan membutuhkan memori lebih besar\n",
    "\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# Load model dengan adapter\n",
    "merged_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    final_output_dir,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Merge\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "# Save merged model\n",
    "merged_output_dir = f\"{training_args.output_dir}/merged_model\"\n",
    "merged_model.save_pretrained(merged_output_dir)\n",
    "tokenizer.save_pretrained(merged_output_dir)\n",
    "\n",
    "print(f\"Merged model saved to: {merged_output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
