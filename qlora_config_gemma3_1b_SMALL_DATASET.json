{
  "dataset_stats": {
    "total_samples": 3855,
    "unique_questions": 771,
    "variations_per_question": 5,
    "train_samples": 3084,
    "eval_samples": 578,
    "test_samples": 193,
    "dataset_type": "small_domain_specific"
  },
  "model_config": {
    "model_name": "google/gemma-3-1b-it",
    "use_cache": false,
    "trust_remote_code": true,
    "torch_dtype": "bfloat16",
    "attn_implementation": "eager"
  },
  "quantization_config": {
    "load_in_4bit": true,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true
  },
  "qlora_config": {
    "r": 32,
    "lora_alpha": 64,
    "lora_dropout": 0.1,
    "bias": "none",
    "task_type": "CAUSAL_LM",
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj",
      "lm_head"
    ],
    "modules_to_save": ["embed_tokens"]
  },
  "dataset_config": {
    "train_file": "data/train.jsonl",
    "eval_file": "data/eval.jsonl",
    "max_length": 1024,
    "packing": false
  },
  "training_args": {
    "output_dir": "./outputs/gemma3-1b-unsiq-small-data",
    "overwrite_output_dir": true,
    "num_train_epochs": 2,
    "per_device_train_batch_size": 4,
    "per_device_eval_batch_size": 8,
    "gradient_accumulation_steps": 4,
    "gradient_checkpointing": true,
    "gradient_checkpointing_kwargs": {"use_reentrant": false},
    "optim": "paged_adamw_8bit",
    "learning_rate": 5e-5,
    "weight_decay": 0.05,
    "max_grad_norm": 0.3,
    "lr_scheduler_type": "cosine",
    "warmup_ratio": 0.05,
    "warmup_steps": 50,
    "eval_strategy": "steps",
    "eval_steps": 25,
    "save_strategy": "steps",
    "save_steps": 25,
    "save_total_limit": null,
    "load_best_model_at_end": true,
    "metric_for_best_model": "eval_loss",
    "greater_is_better": false,
    "logging_strategy": "steps",
    "logging_steps": 5,
    "logging_first_step": true,
    "report_to": ["tensorboard"],
    "bf16": true,
    "bf16_full_eval": true,
    "dataloader_num_workers": 4,
    "dataloader_pin_memory": true,
    "group_by_length": true,
    "ddp_find_unused_parameters": false,
    "seed": 42,
    "data_seed": 42,
    "early_stopping_patience": 5,
    "early_stopping_threshold": 0.001
  },
  "generation_config": {
    "max_new_tokens": 512,
    "temperature": 0.7,
    "top_p": 0.9,
    "top_k": 50,
    "repetition_penalty": 1.1,
    "do_sample": true
  },
  "overfitting_prevention": {
    "techniques_applied": [
      "Lower LoRA rank (32 vs 64) - Reduce model capacity to prevent memorization",
      "Higher dropout (0.1 vs 0.05) - Stronger regularization during training",
      "Higher weight decay (0.05 vs 0.01) - L2 regularization to prevent large weights",
      "Reduced epochs (2 vs 3) - Less training iterations to avoid overfitting",
      "Conservative learning rate (5e-5 vs 2e-4) - Slower, more stable convergence",
      "Early stopping (patience=5) - Stop when validation loss stops improving",
      "Frequent evaluation (every 25 steps) - Monitor overfitting closely",
      "15% validation split - Adequate validation set for monitoring",
      "Group by length - Reduce padding, improve efficiency"
    ]
  },
  "research_based_on": {
    "papers": [
      "Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small LLMs (Dec 2025)",
      "Fine-tuning Large Language Models with Limited Data: A Survey (Nov 2024)",
      "Practical Tips for Finetuning LLMs Using LoRA (Raschka)",
      "LoRA Insights from Hundreds of Experiments (Lightning AI)"
    ],
    "key_findings": [
      "1-3 epochs max for small datasets (we use 2)",
      "Learning rate 1e-4 to 5e-5 for small data (we use 5e-5)",
      "LoRA rank 16-32 optimal for <5000 samples (we use 32)",
      "Dropout 0.1-0.2 for small datasets (we use 0.1)",
      "Weight decay 0.05-0.1 for regularization (we use 0.05)",
      "Quality > Quantity: 500-2000 quality samples beats 10k low quality",
      "Larger batch size + lower LR = better performance",
      "Early stopping critical for small datasets",
      "Frequent validation monitoring essential"
    ]
  },
  "notes": {
    "config_optimized_for": "Gemma 3 1B - Small Dataset (3,855 samples)",
    "hardware": "NVIDIA A100 / T4 / L4",
    "effective_batch_size": 16,
    "total_checkpoints": "unlimited (save all)",
    "expected_training_time": "30-45 minutes on A100",
    "recommendations": [
      "✓ LoRA rank 32 balanced untuk dataset kecil - tidak overfitting",
      "✓ Dropout 0.1 untuk regularization lebih kuat",
      "✓ Weight decay 0.05 mencegah overfitting",
      "✓ 2 epochs optimal untuk 3.8k samples - research proven",
      "✓ Learning rate 5e-5 konservatif untuk small data",
      "✓ Early stopping patience=5 otomatis stop jika overfitting",
      "✓ Eval setiap 25 steps untuk monitoring ketat",
      "✓ Effective batch 16 (4×4) optimal untuk kualitas",
      "✓ Save semua checkpoint untuk perbandingan"
    ],
    "quality_assurance": [
      "Monitor eval_loss setiap 25 steps",
      "Jika eval_loss naik tapi train_loss turun = OVERFITTING",
      "Early stopping akan auto-stop jika tidak improve 5 evaluations",
      "Best model otomatis tersimpan berdasarkan eval_loss terendah",
      "Test inference setiap checkpoint untuk quality check"
    ]
  }
}
