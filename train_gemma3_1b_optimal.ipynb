{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Gemma 3 1B dengan QLoRA - Konfigurasi Optimal untuk Kualitas Terbaik\n",
    "\n",
    "**Model**: Google Gemma 3 1B-IT (Instruction Tuned)\n",
    "\n",
    "**Optimized for**: Kualitas output terbaik dengan A100 GPU\n",
    "\n",
    "## Spesifikasi Gemma 3 1B:\n",
    "- 1 Billion parameters\n",
    "- Context: 32K tokens input, 8K output\n",
    "- Trained on: 2 trillion tokens, 140+ languages\n",
    "- Native BF16 support (trained on TPU)\n",
    "- Multimodal: Text + Images\n",
    "\n",
    "## Konfigurasi Optimal:\n",
    "- **LoRA Rank**: 64 (high capacity)\n",
    "- **Learning Rate**: 2e-4 (recommended by Google)\n",
    "- **Max Length**: 1024 tokens\n",
    "- **Epochs**: 3 (balanced quality)\n",
    "- **Effective Batch Size**: 16\n",
    "- **All Checkpoints Saved**: Unlimited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U torch>=2.4.0 transformers>=4.50.0 accelerate bitsandbytes peft datasets trl tensorboard sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {__import__('transformers').__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU Memory: {gpu_props.total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU Compute Capability: {gpu_props.major}.{gpu_props.minor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Optimal Configuration for Gemma 3 1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load optimal config\n",
    "config_path = \"qlora_config_gemma3_1b_OPTIMAL.json\"\n",
    "\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTIMAL CONFIGURATION FOR GEMMA 3 1B - BEST QUALITY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nModel: {config['model_config']['model_name']}\")\n",
    "print(f\"\\nLoRA Configuration:\")\n",
    "print(f\"  - Rank (r): {config['qlora_config']['r']}\")\n",
    "print(f\"  - Alpha: {config['qlora_config']['lora_alpha']}\")\n",
    "print(f\"  - Dropout: {config['qlora_config']['lora_dropout']}\")\n",
    "print(f\"  - Target modules: {', '.join(config['qlora_config']['target_modules'])}\")\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  - Learning Rate: {config['training_args']['learning_rate']}\")\n",
    "print(f\"  - Epochs: {config['training_args']['num_train_epochs']}\")\n",
    "print(f\"  - Batch Size per Device: {config['training_args']['per_device_train_batch_size']}\")\n",
    "print(f\"  - Gradient Accumulation: {config['training_args']['gradient_accumulation_steps']}\")\n",
    "print(f\"  - Effective Batch Size: {config['training_args']['per_device_train_batch_size'] * config['training_args']['gradient_accumulation_steps']}\")\n",
    "print(f\"  - Max Sequence Length: {config['dataset_config']['max_length']} tokens\")\n",
    "print(f\"  - Save Steps: {config['training_args']['save_steps']}\")\n",
    "print(f\"  - Save Total Limit: Unlimited (all checkpoints saved)\")\n",
    "print(\"\\nRecommendations:\")\n",
    "for note in config['notes']['recommendations']:\n",
    "    print(f\"  ‚úì {note}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup Model & Tokenizer dengan QLoRA 4-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config['model_config']['model_name']\n",
    "\n",
    "# Quantization config untuk 4-bit NF4 dengan double quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=config['quantization_config']['load_in_4bit'],\n",
    "    bnb_4bit_compute_dtype=getattr(torch, config['quantization_config']['bnb_4bit_compute_dtype']),\n",
    "    bnb_4bit_quant_type=config['quantization_config']['bnb_4bit_quant_type'],\n",
    "    bnb_4bit_use_double_quant=config['quantization_config']['bnb_4bit_use_double_quant'],\n",
    ")\n",
    "\n",
    "print(f\"\\nLoading Gemma 3 1B model: {model_name}\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "# Load model dengan 4-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=config['model_config']['trust_remote_code'],\n",
    "    torch_dtype=getattr(torch, config['model_config']['torch_dtype']),\n",
    "    use_cache=config['model_config']['use_cache'],\n",
    "    attn_implementation=config['model_config'].get('attn_implementation', 'eager'),\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Set padding\n",
    "tokenizer.padding_side = 'right'\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"\\n‚úì Model loaded successfully!\")\n",
    "print(f\"  Total parameters: {model.num_parameters():,}\")\n",
    "print(f\"  Tokenizer vocab size: {len(tokenizer):,}\")\n",
    "print(f\"  Model dtype: {model.dtype}\")\n",
    "print(f\"  Device map: {model.hf_device_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup LoRA - High Rank untuk Kualitas Maksimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for k-bit training\n",
    "print(\"Preparing model for QLoRA training...\")\n",
    "model = prepare_model_for_kbit_training(\n",
    "    model,\n",
    "    use_gradient_checkpointing=config['training_args']['gradient_checkpointing']\n",
    ")\n",
    "\n",
    "# LoRA config dengan rank tinggi untuk kualitas terbaik\n",
    "lora_config = LoraConfig(\n",
    "    r=config['qlora_config']['r'],\n",
    "    lora_alpha=config['qlora_config']['lora_alpha'],\n",
    "    lora_dropout=config['qlora_config']['lora_dropout'],\n",
    "    bias=config['qlora_config']['bias'],\n",
    "    task_type=config['qlora_config']['task_type'],\n",
    "    target_modules=config['qlora_config']['target_modules'],\n",
    "    modules_to_save=config['qlora_config'].get('modules_to_save', None),\n",
    ")\n",
    "\n",
    "# Apply LoRA adapters\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Calculate trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_percent = 100 * trainable_params / all_params\n",
    "\n",
    "print(f\"\\n‚úì LoRA adapters applied!\")\n",
    "print(f\"\\nTrainable parameters breakdown:\")\n",
    "print(f\"  Trainable params: {trainable_params:,}\")\n",
    "print(f\"  All params: {all_params:,}\")\n",
    "print(f\"  Trainable%: {trainable_percent:.4f}%\")\n",
    "print(f\"\\nWith LoRA rank {config['qlora_config']['r']}, you have HIGH capacity for learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Dataset UNSIQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print(\"Loading UNSIQ datasets...\\n\")\n",
    "\n",
    "train_dataset = load_dataset(\n",
    "    'json',\n",
    "    data_files=config['dataset_config']['train_file'],\n",
    "    split='train'\n",
    ")\n",
    "\n",
    "eval_dataset = load_dataset(\n",
    "    'json',\n",
    "    data_files=config['dataset_config']['eval_file'],\n",
    "    split='train'\n",
    ")\n",
    "\n",
    "print(f\"‚úì Datasets loaded:\")\n",
    "print(f\"  Train: {len(train_dataset):,} samples\")\n",
    "print(f\"  Eval: {len(eval_dataset):,} samples\")\n",
    "print(f\"  Total: {len(train_dataset) + len(eval_dataset):,} samples\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample data structure:\")\n",
    "print(json.dumps(train_dataset[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Format Dataset dengan Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_template(example):\n",
    "    \"\"\"\n",
    "    Format messages menggunakan Gemma 3 chat template\n",
    "    \"\"\"\n",
    "    messages = example['messages']\n",
    "    \n",
    "    # Apply tokenizer's chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    return {'text': text}\n",
    "\n",
    "print(\"Formatting datasets with Gemma 3 chat template...\")\n",
    "\n",
    "# Apply formatting\n",
    "train_dataset = train_dataset.map(\n",
    "    format_chat_template,\n",
    "    desc=\"Formatting train dataset\"\n",
    ")\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    format_chat_template,\n",
    "    desc=\"Formatting eval dataset\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Datasets formatted!\")\n",
    "print(\"\\nFormatted example (first 500 chars):\")\n",
    "print(\"-\" * 80)\n",
    "print(train_dataset[0]['text'][:500])\n",
    "print(\"...\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Configuration - Optimized for Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_config = config['training_args']\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(training_args_config['output_dir'], exist_ok=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    # Output\n",
    "    output_dir=training_args_config['output_dir'],\n",
    "    overwrite_output_dir=training_args_config['overwrite_output_dir'],\n",
    "    \n",
    "    # Training\n",
    "    num_train_epochs=training_args_config['num_train_epochs'],\n",
    "    per_device_train_batch_size=training_args_config['per_device_train_batch_size'],\n",
    "    per_device_eval_batch_size=training_args_config['per_device_eval_batch_size'],\n",
    "    gradient_accumulation_steps=training_args_config['gradient_accumulation_steps'],\n",
    "    gradient_checkpointing=training_args_config['gradient_checkpointing'],\n",
    "    gradient_checkpointing_kwargs=training_args_config.get('gradient_checkpointing_kwargs', {}),\n",
    "    \n",
    "    # Optimization\n",
    "    optim=training_args_config['optim'],\n",
    "    learning_rate=training_args_config['learning_rate'],\n",
    "    weight_decay=training_args_config['weight_decay'],\n",
    "    max_grad_norm=training_args_config['max_grad_norm'],\n",
    "    \n",
    "    # Scheduler\n",
    "    lr_scheduler_type=training_args_config['lr_scheduler_type'],\n",
    "    warmup_ratio=training_args_config['warmup_ratio'],\n",
    "    warmup_steps=training_args_config.get('warmup_steps', 0),\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=training_args_config['eval_strategy'],\n",
    "    eval_steps=training_args_config['eval_steps'],\n",
    "    \n",
    "    # Checkpointing - SAVE ALL\n",
    "    save_strategy=training_args_config['save_strategy'],\n",
    "    save_steps=training_args_config['save_steps'],\n",
    "    save_total_limit=training_args_config['save_total_limit'],  # None = unlimited\n",
    "    load_best_model_at_end=training_args_config['load_best_model_at_end'],\n",
    "    metric_for_best_model=training_args_config['metric_for_best_model'],\n",
    "    greater_is_better=training_args_config.get('greater_is_better', False),\n",
    "    \n",
    "    # Logging\n",
    "    logging_strategy=training_args_config['logging_strategy'],\n",
    "    logging_steps=training_args_config['logging_steps'],\n",
    "    logging_first_step=training_args_config.get('logging_first_step', True),\n",
    "    report_to=training_args_config['report_to'],\n",
    "    \n",
    "    # Mixed Precision\n",
    "    bf16=training_args_config['bf16'],\n",
    "    bf16_full_eval=training_args_config['bf16_full_eval'],\n",
    "    \n",
    "    # Data loading\n",
    "    dataloader_num_workers=training_args_config['dataloader_num_workers'],\n",
    "    dataloader_pin_memory=training_args_config.get('dataloader_pin_memory', True),\n",
    "    group_by_length=training_args_config['group_by_length'],\n",
    "    \n",
    "    # DDP\n",
    "    ddp_find_unused_parameters=training_args_config['ddp_find_unused_parameters'],\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=training_args_config.get('seed', 42),\n",
    "    data_seed=training_args_config.get('data_seed', 42),\n",
    ")\n",
    "\n",
    "# Calculate training stats\n",
    "steps_per_epoch = len(train_dataset) // (\n",
    "    training_args.per_device_train_batch_size * \n",
    "    training_args.gradient_accumulation_steps\n",
    ")\n",
    "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
    "num_checkpoints = total_steps // training_args.save_steps\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING PLAN\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Warmup steps: {int(total_steps * training_args.warmup_ratio)}\")\n",
    "print(f\"Eval every: {training_args.eval_steps} steps\")\n",
    "print(f\"Save checkpoint every: {training_args.save_steps} steps\")\n",
    "print(f\"Expected checkpoints: ~{num_checkpoints}\")\n",
    "print(f\"Save limit: {'ALL CHECKPOINTS' if training_args.save_total_limit is None else training_args.save_total_limit}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Initialize SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing SFTTrainer...\\n\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field='text',\n",
    "    max_seq_length=config['dataset_config']['max_length'],\n",
    "    packing=config['dataset_config'].get('packing', False),\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer initialized successfully!\")\n",
    "print(\"\\nReady to start training...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Start Training - Monitoring Real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"STARTING TRAINING - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTraining Gemma 3 1B on UNSIQ dataset...\")\n",
    "print(\"Monitor progress in TensorBoard: tensorboard --logdir \" + training_args.output_dir)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Start training\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"TRAINING COMPLETED - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFinal Training Loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Training Runtime: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Samples per second: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
    "print(f\"Steps per second: {train_result.metrics['train_steps_per_second']:.2f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Final Model & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final adapter\n",
    "final_output_dir = f\"{training_args.output_dir}/final_adapter\"\n",
    "print(f\"Saving final adapter to: {final_output_dir}\")\n",
    "\n",
    "trainer.model.save_pretrained(final_output_dir)\n",
    "tokenizer.save_pretrained(final_output_dir)\n",
    "\n",
    "# Save training config\n",
    "config_save_path = f\"{training_args.output_dir}/training_config.json\"\n",
    "with open(config_save_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Save training metrics\n",
    "metrics_file = f\"{training_args.output_dir}/training_metrics.json\"\n",
    "with open(metrics_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_result.metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì Final adapter saved: {final_output_dir}\")\n",
    "print(f\"‚úì Training config saved: {config_save_path}\")\n",
    "print(f\"‚úì Training metrics saved: {metrics_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRunning final evaluation...\\n\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save evaluation results\n",
    "eval_file = f\"{training_args.output_dir}/final_eval_results.json\"\n",
    "with open(eval_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(eval_results, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì Evaluation results saved: {eval_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Test Inference - Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question, max_new_tokens=512):\n",
    "    \"\"\"\n",
    "    Generate response using fine-tuned model\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Anda adalah asisten informasi UNSIQ (Universitas Sains Al-Qur'an) yang membantu menjawab pertanyaan tentang biaya kuliah, program studi, dan informasi akademik.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Format dengan chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    gen_config = config.get('generation_config', {})\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=gen_config.get('max_new_tokens', max_new_tokens),\n",
    "            temperature=gen_config.get('temperature', 0.7),\n",
    "            top_p=gen_config.get('top_p', 0.9),\n",
    "            top_k=gen_config.get('top_k', 50),\n",
    "            repetition_penalty=gen_config.get('repetition_penalty', 1.1),\n",
    "            do_sample=gen_config.get('do_sample', True),\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode hanya generated tokens\n",
    "    generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "# Test dengan berbagai pertanyaan\n",
    "test_questions = [\n",
    "    \"Berapa total biaya kuliah S1 Akuntansi di UNSIQ untuk 8 semester?\",\n",
    "    \"Apa itu KIP Kuliah dan bagaimana cara mendaftarnya?\",\n",
    "    \"Mengapa biaya semester 1 lebih mahal dari semester lainnya?\",\n",
    "    \"Program studi apa saja yang tersedia di UNSIQ?\",\n",
    "    \"Bagaimana sistem pembayaran kuliah di UNSIQ?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING FINE-TUNED MODEL - QUALITY CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TEST {i}/{len(test_questions)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n‚ùì QUESTION: {question}\")\n",
    "    print(f\"\\nü§ñ ANSWER:\")\n",
    "    \n",
    "    response = generate_response(question)\n",
    "    print(response)\n",
    "    print(f\"\\n{'-'*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. List All Saved Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = training_args.output_dir\n",
    "checkpoints = sorted(\n",
    "    [d for d in os.listdir(output_dir) if d.startswith('checkpoint-')],\n",
    "    key=lambda x: int(x.split('-')[-1])\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"ALL SAVED CHECKPOINTS ({len(checkpoints)} total)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_size = 0\n",
    "for i, cp in enumerate(checkpoints, 1):\n",
    "    cp_path = os.path.join(output_dir, cp)\n",
    "    \n",
    "    # Calculate size\n",
    "    size = 0\n",
    "    for root, dirs, files in os.walk(cp_path):\n",
    "        size += sum(os.path.getsize(os.path.join(root, f)) for f in files)\n",
    "    \n",
    "    total_size += size\n",
    "    step_num = int(cp.split('-')[-1])\n",
    "    \n",
    "    print(f\"{i:3d}. {cp:20s} | Step {step_num:5d} | Size: {size / 1024**2:7.2f} MB\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"Total checkpoint storage: {total_size / 1024**3:.2f} GB\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüí° TIP: Anda dapat load checkpoint tertentu dengan:\")\n",
    "print(f\"   from peft import AutoPeftModelForCausalLM\")\n",
    "print(f\"   model = AutoPeftModelForCausalLM.from_pretrained('{output_dir}/checkpoint-XXX')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. TensorBoard Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tensorboard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "print(\"Opening TensorBoard...\\n\")\n",
    "print(\"You can view:\")\n",
    "print(\"  ‚Ä¢ Training loss curve\")\n",
    "print(\"  ‚Ä¢ Evaluation loss curve\")\n",
    "print(\"  ‚Ä¢ Learning rate schedule\")\n",
    "print(\"  ‚Ä¢ Gradient norms\")\n",
    "print(\"\\nMonitor all metrics to ensure quality training!\\n\")\n",
    "\n",
    "%tensorboard --logdir {training_args.output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Compare Checkpoint Performance (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk evaluate checkpoint tertentu\n",
    "def evaluate_checkpoint(checkpoint_path, test_questions):\n",
    "    \"\"\"\n",
    "    Load dan test checkpoint tertentu\n",
    "    \"\"\"\n",
    "    from peft import AutoPeftModelForCausalLM\n",
    "    \n",
    "    print(f\"\\nLoading checkpoint: {checkpoint_path}\")\n",
    "    \n",
    "    # Load model dari checkpoint\n",
    "    temp_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        checkpoint_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    for q in test_questions:\n",
    "        # Generate response (gunakan fungsi yang sama)\n",
    "        response = generate_response(q)\n",
    "        results.append({\n",
    "            'question': q,\n",
    "            'answer': response\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Compare first, middle, and last checkpoint\n",
    "print(\"\\nTo compare checkpoints, uncomment and run:\")\n",
    "print(\"\"\"# checkpoints_to_compare = [\n",
    "#     f\"{output_dir}/checkpoint-50\",  # Early\n",
    "#     f\"{output_dir}/checkpoint-{len(checkpoints)//2 * 50}\",  # Middle\n",
    "#     f\"{output_dir}/final_adapter\",  # Final\n",
    "# ]\n",
    "# \n",
    "# for cp in checkpoints_to_compare:\n",
    "#     if os.path.exists(cp):\n",
    "#         results = evaluate_checkpoint(cp, test_questions[:2])\n",
    "#         print(f\"\\\\n{cp}:\")\n",
    "#         for r in results:\n",
    "#             print(f\"Q: {r['question']}\")\n",
    "#             print(f\"A: {r['answer'][:200]}...\\\\n\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Merge LoRA Adapters untuk Deployment (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: Merge membutuhkan RAM/VRAM lebih besar!\n",
    "# Hasilnya adalah model lengkap tanpa perlu load adapter terpisah\n",
    "\n",
    "print(\"\\nMerging LoRA adapters dengan base model...\")\n",
    "print(\"‚ö†Ô∏è  WARNING: This requires significant memory!\\n\")\n",
    "\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# Load model dengan adapter\n",
    "print(\"Loading model with adapters...\")\n",
    "merged_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    final_output_dir,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(\"Merging adapters into base model...\")\n",
    "# Merge adapters\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "# Save merged model\n",
    "merged_output_dir = f\"{training_args.output_dir}/merged_model_full\"\n",
    "print(f\"Saving merged model to: {merged_output_dir}\")\n",
    "\n",
    "merged_model.save_pretrained(\n",
    "    merged_output_dir,\n",
    "    safe_serialization=True,\n",
    "    max_shard_size=\"2GB\"\n",
    ")\n",
    "tokenizer.save_pretrained(merged_output_dir)\n",
    "\n",
    "print(f\"\\n‚úì Merged model saved to: {merged_output_dir}\")\n",
    "print(\"\\nYou can now use this model directly without loading adapters!\")\n",
    "print(f\"Model size: ~{sum(os.path.getsize(os.path.join(merged_output_dir, f)) for f in os.listdir(merged_output_dir)) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Training Summary & Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING SUMMARY - GEMMA 3 1B OPTIMAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = f\"\"\"\n",
    "Model: {config['model_config']['model_name']}\n",
    "Dataset: UNSIQ ({len(train_dataset)} train, {len(eval_dataset)} eval)\n",
    "\n",
    "LoRA Configuration:\n",
    "  ‚Ä¢ Rank: {config['qlora_config']['r']} (HIGH capacity)\n",
    "  ‚Ä¢ Alpha: {config['qlora_config']['lora_alpha']}\n",
    "  ‚Ä¢ Target modules: {len(config['qlora_config']['target_modules'])} layers\n",
    "  ‚Ä¢ Trainable params: {trainable_params:,} ({trainable_percent:.4f}%)\n",
    "\n",
    "Training Configuration:\n",
    "  ‚Ä¢ Learning rate: {config['training_args']['learning_rate']}\n",
    "  ‚Ä¢ Effective batch size: {config['training_args']['per_device_train_batch_size'] * config['training_args']['gradient_accumulation_steps']}\n",
    "  ‚Ä¢ Epochs: {config['training_args']['num_train_epochs']}\n",
    "  ‚Ä¢ Max sequence length: {config['dataset_config']['max_length']} tokens\n",
    "  ‚Ä¢ Total steps: {total_steps}\n",
    "\n",
    "Results:\n",
    "  ‚Ä¢ Final training loss: {train_result.training_loss:.4f}\n",
    "  ‚Ä¢ Final eval loss: {eval_results.get('eval_loss', 'N/A')}\n",
    "  ‚Ä¢ Training time: {train_result.metrics['train_runtime']/60:.2f} minutes\n",
    "  ‚Ä¢ Checkpoints saved: {len(checkpoints)}\n",
    "\n",
    "Output Files:\n",
    "  üìÅ {training_args.output_dir}/\n",
    "     ‚îú‚îÄ‚îÄ final_adapter/ (LoRA weights)\n",
    "     ‚îú‚îÄ‚îÄ checkpoint-*/ (all training checkpoints)\n",
    "     ‚îú‚îÄ‚îÄ merged_model_full/ (merged model - if created)\n",
    "     ‚îú‚îÄ‚îÄ training_config.json\n",
    "     ‚îú‚îÄ‚îÄ training_metrics.json\n",
    "     ‚îî‚îÄ‚îÄ final_eval_results.json\n",
    "\n",
    "Next Steps:\n",
    "  1. Review TensorBoard metrics untuk analisis training\n",
    "  2. Test model dengan lebih banyak pertanyaan\n",
    "  3. Jika perlu, load checkpoint tertentu untuk perbandingan\n",
    "  4. Deploy model (gunakan final_adapter atau merged_model)\n",
    "\n",
    "üéâ TRAINING COMPLETE - MODEL READY FOR DEPLOYMENT!\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save summary\n",
    "summary_file = f\"{training_args.output_dir}/TRAINING_SUMMARY.txt\"\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(f\"\\n‚úì Summary saved to: {summary_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
