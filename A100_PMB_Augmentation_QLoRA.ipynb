{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ A100 80GB - PMB Augmentation + QLoRA Training\n",
        "## Enhanced with Comprehensive Validation & Semantic Preservation\n",
        "\n",
        "**Complete Notebook untuk Augmentasi + Training**\n",
        "- ‚ö° Ultra-fast augmentation (30-40s untuk 1452 items)\n",
        "- ‚úÖ Multi-layer validation (Basic + Semantic + Domain + Dedup)\n",
        "- üß† Semantic meaning preservation dengan embeddings\n",
        "- üéì Full QLoRA training pipeline\n",
        "- üìä Comprehensive quality metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install all dependencies\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q transformers accelerate bitsandbytes\n",
        "!pip install -q huggingface-hub datasets tqdm\n",
        "!pip install -q flash-attn --no-build-isolation\n",
        "!pip install -q peft trl sentence-transformers\n",
        "!pip install -q scikit-learn pandas\n",
        "\n",
        "import torch\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from tqdm.auto import tqdm\n",
        "from difflib import SequenceMatcher\n",
        "import re\n",
        "from dataclasses import dataclass, asdict, field\n",
        "from huggingface_hub import hf_hub_download\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(f'‚úÖ CUDA: {torch.cuda.is_available()}')\n",
        "print(f'‚úÖ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'‚úÖ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB')\n",
        "    print(f'‚úÖ Compute: {torch.cuda.get_device_capability(0)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ Configuration (A100 Optimized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class A100_EnhancedConfig:\n",
        "    '''Maximum speed + quality untuk A100 80GB'''\n",
        "    \n",
        "    # GPU Configuration\n",
        "    device: str = 'cuda:0'\n",
        "    torch_dtype = torch.bfloat16\n",
        "    \n",
        "    # Model Configuration\n",
        "    model_name: str = 'google/gemma-3-1b-instruct'\n",
        "    training_model: str = 'google/gemma-3-1b'\n",
        "    load_in_4bit: bool = False\n",
        "    use_cache: bool = True\n",
        "    \n",
        "    # Batch Processing\n",
        "    batch_size: int = 128\n",
        "    num_workers: int = 8\n",
        "    \n",
        "    # Generation\n",
        "    max_new_tokens: int = 200\n",
        "    temperature: float = 0.5\n",
        "    top_p: float = 0.85\n",
        "    top_k: int = 50\n",
        "    repetition_penalty: float = 1.05\n",
        "    \n",
        "    # Quality Validation\n",
        "    max_q_similarity: float = 0.85\n",
        "    min_q_length: int = 10\n",
        "    max_q_length: int = 250\n",
        "    min_q_words: int = 4\n",
        "    max_q_words: int = 25\n",
        "    \n",
        "    # Semantic Validation\n",
        "    semantic_model: str = 'distiluse-base-multilingual-cased-v2'\n",
        "    semantic_similarity_threshold: float = 0.75\n",
        "    meaning_drift_threshold: float = 0.60\n",
        "    \n",
        "    # Deduplication\n",
        "    duplicate_threshold: float = 0.90\n",
        "    \n",
        "    # QLoRA Training\n",
        "    lora_r: int = 16\n",
        "    lora_alpha: int = 32\n",
        "    lora_dropout: float = 0.05\n",
        "    target_modules: List[str] = field(default_factory=lambda: ['q_proj', 'v_proj'])\n",
        "    \n",
        "    # Training Arguments\n",
        "    num_train_epochs: int = 3\n",
        "    per_device_train_batch_size: int = 4\n",
        "    per_device_eval_batch_size: int = 4\n",
        "    gradient_accumulation_steps: int = 2\n",
        "    learning_rate: float = 2e-4\n",
        "    warmup_steps: int = 100\n",
        "    max_grad_norm: float = 1.0\n",
        "    weight_decay: float = 0.01\n",
        "    \n",
        "    # Evaluation\n",
        "    eval_steps: int = 100\n",
        "    save_steps: int = 200\n",
        "    save_total_limit: int = 3\n",
        "    load_best_model_at_end: bool = True\n",
        "    \n",
        "    # Monitoring\n",
        "    log_memory: bool = True\n",
        "    save_every_n_batches: int = 10\n",
        "\n",
        "config = A100_EnhancedConfig()\n",
        "\n",
        "print('\\n' + '='*70)\n",
        "print('‚öôÔ∏è CONFIG - A100 80GB WITH ENHANCED VALIDATION')\n",
        "print('='*70)\n",
        "print(f'Model: {config.model_name}')\n",
        "print(f'Batch size: {config.batch_size}')\n",
        "print(f'Semantic model: {config.semantic_model}')\n",
        "print(f'Meaning drift threshold: {config.meaning_drift_threshold}')\n",
        "print(f'Training epochs: {config.num_train_epochs}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3Ô∏è‚É£ GPU Monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GPUMonitor:\n",
        "    '''Real-time GPU memory monitoring'''\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.max_allocated = 0\n",
        "        self.max_reserved = 0\n",
        "        self.stage_history = []\n",
        "    \n",
        "    def log(self, stage: str = '', detailed: bool = False):\n",
        "        if not torch.cuda.is_available():\n",
        "            return\n",
        "        \n",
        "        allocated = torch.cuda.memory_allocated() / 1e9\n",
        "        reserved = torch.cuda.memory_reserved() / 1e9\n",
        "        max_allocated = torch.cuda.max_memory_allocated() / 1e9\n",
        "        \n",
        "        self.max_allocated = max(self.max_allocated, max_allocated)\n",
        "        self.max_reserved = max(self.max_reserved, reserved)\n",
        "        \n",
        "        self.stage_history.append({\n",
        "            'stage': stage,\n",
        "            'allocated': allocated,\n",
        "            'reserved': reserved\n",
        "        })\n",
        "        \n",
        "        if config.log_memory:\n",
        "            print(f'[{stage:20}] Alloc: {allocated:6.1f}GB | Reserved: {reserved:6.1f}GB | Peak: {max_allocated:6.1f}GB')\n",
        "    \n",
        "    def summary(self):\n",
        "        print(f'\\nüìä GPU Memory Summary:')\n",
        "        print(f' Peak allocated: {self.max_allocated:.1f}GB / 80GB')\n",
        "        print(f' Peak reserved: {self.max_reserved:.1f}GB / 80GB')\n",
        "        print(f' Utilization: {self.max_allocated/80*100:.1f}%')\n",
        "\n",
        "gpu_monitor = GPUMonitor()\n",
        "print('‚úÖ GPU Monitor initialized')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ Semantic Validator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SemanticValidator:\n",
        "    '''Semantic-level validation untuk menjaga makna'''\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        print(f'\\nüì• Loading semantic model: {config.semantic_model}...')\n",
        "        self.model = SentenceTransformer(config.semantic_model)\n",
        "        self.stats = defaultdict(int)\n",
        "    \n",
        "    def get_embeddings(self, texts: List[str]) -> np.ndarray:\n",
        "        '''Get semantic embeddings'''\n",
        "        return self.model.encode(texts, show_progress_bar=False)\n",
        "    \n",
        "    def check_meaning_preservation(self, original_q: str, variation_q: str,\n",
        "                                   original_embedding) -> Tuple[bool, float]:\n",
        "        '''Check jika variation mempertahankan makna core'''\n",
        "        variation_embedding = self.get_embeddings([variation_q])[0]\n",
        "        similarity = cosine_similarity([original_embedding], [variation_embedding])[0][0]\n",
        "        is_valid = similarity >= self.config.meaning_drift_threshold\n",
        "        \n",
        "        if is_valid:\n",
        "            self.stats['valid'] += 1\n",
        "        else:\n",
        "            self.stats['rejected'] += 1\n",
        "        \n",
        "        return is_valid, similarity\n",
        "    \n",
        "    def validate_batch(self, original_q: str, variations: List[str]) -> Dict:\n",
        "        '''Batch semantic validation'''\n",
        "        results = {'valid': [], 'scores': [], 'rejected': []}\n",
        "        original_embedding = self.get_embeddings([original_q])[0]\n",
        "        \n",
        "        for var in variations:\n",
        "            is_valid, score = self.check_meaning_preservation(original_q, var, original_embedding)\n",
        "            if is_valid:\n",
        "                results['valid'].append(var)\n",
        "                results['scores'].append(score)\n",
        "            else:\n",
        "                results['rejected'].append((var, score))\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def report(self):\n",
        "        total = self.stats['valid'] + self.stats['rejected']\n",
        "        if total == 0:\n",
        "            return '0/0 (0%)'\n",
        "        pct = self.stats['valid'] / total * 100\n",
        "        return f\"{self.stats['valid']}/{total} ({pct:.1f}%)\"\n",
        "\n",
        "semantic_validator = SemanticValidator(config)\n",
        "print('‚úÖ Semantic Validator initialized')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5Ô∏è‚É£ Other Validators (Dedup + Domain + Quality)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DeduplicationValidator:\n",
        "    '''Deteksi near-duplicate variations'''\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.seen_questions = {}\n",
        "        self.stats = defaultdict(int)\n",
        "    \n",
        "    def is_duplicate(self, question: str, threshold: float = None) -> Tuple[bool, float]:\n",
        "        if threshold is None:\n",
        "            threshold = self.config.duplicate_threshold\n",
        "        \n",
        "        question_lower = question.lower().strip()\n",
        "        \n",
        "        if not self.seen_questions:\n",
        "            self.seen_questions[question_lower] = question\n",
        "            return False, 1.0\n",
        "        \n",
        "        max_similarity = 0\n",
        "        for seen_q in self.seen_questions.keys():\n",
        "            sim = SequenceMatcher(None, question_lower, seen_q).ratio()\n",
        "            max_similarity = max(max_similarity, sim)\n",
        "        \n",
        "        is_dup = max_similarity >= threshold\n",
        "        \n",
        "        if not is_dup:\n",
        "            self.seen_questions[question_lower] = question\n",
        "            self.stats['unique'] += 1\n",
        "        else:\n",
        "            self.stats['duplicates'] += 1\n",
        "        \n",
        "        return is_dup, max_similarity\n",
        "    \n",
        "    def reset(self):\n",
        "        self.seen_questions = {}\n",
        "    \n",
        "    def report(self):\n",
        "        total = self.stats['unique'] + self.stats['duplicates']\n",
        "        if total == 0:\n",
        "            return '0/0 (0%)'\n",
        "        pct = self.stats['unique'] / total * 100\n",
        "        return f\"{self.stats['unique']}/{total} unique ({pct:.1f}%)\"\n",
        "\n",
        "\n",
        "class PMBDomainValidator:\n",
        "    '''Validasi domain-specific untuk PMB'''\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.pmb_keywords = {\n",
        "            'biaya': ['biaya', 'bayar', 'cicilan', 'uang'],\n",
        "            'program': ['program', 'jurusan', 'fakultas', 'prodi'],\n",
        "            'persyaratan': ['syarat', 'requirement', 'kriteria'],\n",
        "            'pendaftaran': ['daftar', 'registrasi', 'aplikasi'],\n",
        "            'jadwal': ['jadwal', 'tanggal', 'kapan', 'waktu'],\n",
        "            'dokumen': ['dokumen', 'berkas', 'file', 'surat']\n",
        "        }\n",
        "        self.stats = defaultdict(int)\n",
        "    \n",
        "    def extract_entities(self, text: str) -> List[str]:\n",
        "        text_lower = text.lower()\n",
        "        found_entities = []\n",
        "        \n",
        "        for category, keywords in self.pmb_keywords.items():\n",
        "            for keyword in keywords:\n",
        "                if keyword in text_lower:\n",
        "                    found_entities.append(category)\n",
        "                    break\n",
        "        \n",
        "        return list(set(found_entities))\n",
        "    \n",
        "    def validate_entity_preservation(self, original_q: str, variation_q: str) -> Tuple[bool, List[str]]:\n",
        "        orig_entities = self.extract_entities(original_q)\n",
        "        var_entities = self.extract_entities(variation_q)\n",
        "        \n",
        "        if len(orig_entities) == 0:\n",
        "            is_valid = True\n",
        "            lost_entities = []\n",
        "        else:\n",
        "            preserved = len(set(orig_entities) & set(var_entities))\n",
        "            preservation_ratio = preserved / len(orig_entities)\n",
        "            is_valid = preservation_ratio >= 0.8\n",
        "            lost_entities = list(set(orig_entities) - set(var_entities))\n",
        "        \n",
        "        if is_valid:\n",
        "            self.stats['valid'] += 1\n",
        "        else:\n",
        "            self.stats['invalid'] += 1\n",
        "        \n",
        "        return is_valid, lost_entities\n",
        "    \n",
        "    def report(self):\n",
        "        total = self.stats['valid'] + self.stats['invalid']\n",
        "        if total == 0:\n",
        "            return '0/0 (0%)'\n",
        "        pct = self.stats['valid'] / total * 100\n",
        "        return f\"{self.stats['valid']}/{total} ({pct:.1f}%)\"\n",
        "\n",
        "\n",
        "class ComprehensiveQualityValidator:\n",
        "    '''Kombinasi dari semua validators'''\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.stats = defaultdict(int)\n",
        "    \n",
        "    def validate_question(self, q: str, original_q: str) -> Tuple[bool, List[str]]:\n",
        "        issues = []\n",
        "        \n",
        "        if len(q) < self.config.min_q_length or len(q) > self.config.max_q_length:\n",
        "            issues.append(f'len:{len(q)}')\n",
        "        \n",
        "        wc = len(q.split())\n",
        "        if wc < self.config.min_q_words or wc > self.config.max_q_words:\n",
        "            issues.append(f'wc:{wc}')\n",
        "        \n",
        "        sim = SequenceMatcher(None, original_q.lower(), q.lower()).ratio()\n",
        "        if sim > self.config.max_q_similarity:\n",
        "            issues.append(f'too_similar:{sim:.2f}')\n",
        "        \n",
        "        if not q.strip().endswith('?'):\n",
        "            issues.append('not_question')\n",
        "        \n",
        "        punct = sum(1 for c in q if c in '!?.,;:')\n",
        "        if punct > 4:\n",
        "            issues.append(f'punct:{punct}')\n",
        "        \n",
        "        is_valid = len(issues) == 0\n",
        "        if is_valid:\n",
        "            self.stats['valid'] += 1\n",
        "        else:\n",
        "            self.stats['invalid'] += 1\n",
        "        \n",
        "        return is_valid, issues\n",
        "\n",
        "\n",
        "# Initialize validators\n",
        "print('\\nüì• Initializing validators...')\n",
        "dedup_validator = DeduplicationValidator(config)\n",
        "pmb_validator = PMBDomainValidator()\n",
        "quality_validator = ComprehensiveQualityValidator(config)\n",
        "print('‚úÖ All validators initialized')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6Ô∏è‚É£ Load Model & Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\nüì• Loading Gemma-3-1B-Instruct...')\n",
        "start = time.time()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    config.model_name,\n",
        "    trust_remote_code=True,\n",
        "    use_fast=True\n",
        ")\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "gpu_monitor.log('Before model load')\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.model_name,\n",
        "    torch_dtype=config.torch_dtype,\n",
        "    device_map=config.device,\n",
        "    load_in_4bit=False,\n",
        "    use_cache=config.use_cache,\n",
        "    trust_remote_code=True,\n",
        "    low_cpu_mem_usage=True,\n",
        "    attn_implementation='flash_attention_2',\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "gpu_monitor.log('After model load')\n",
        "\n",
        "elapsed = time.time() - start\n",
        "print(f'‚úÖ Model loaded in {elapsed:.1f}s')\n",
        "\n",
        "# Load dataset\n",
        "print('\\nüì• Loading PMB dataset...')\n",
        "jsonl_path = hf_hub_download(\n",
        "    repo_id='Pandusu/pmb-v2',\n",
        "    filename='fix.jsonl',\n",
        "    repo_type='dataset'\n",
        ")\n",
        "\n",
        "data = []\n",
        "with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        if line.strip():\n",
        "            data.append(json.loads(line))\n",
        "\n",
        "print(f'‚úÖ Loaded {len(data)} entries')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7Ô∏è‚É£ Enhanced Augmentation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EnhancedAugmentationPipeline:\n",
        "    def __init__(self, config, model, tokenizer, semantic_validator,\n",
        "                 dedup_validator, pmb_validator, quality_validator, gpu_monitor):\n",
        "        self.config = config\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.semantic_val = semantic_validator\n",
        "        self.dedup_val = dedup_validator\n",
        "        self.pmb_val = pmb_validator\n",
        "        self.quality_val = quality_validator\n",
        "        self.gpu_monitor = gpu_monitor\n",
        "        \n",
        "        self.stats = {\n",
        "            'total_processed': 0,\n",
        "            'success': 0,\n",
        "            'failed': 0,\n",
        "            'total_variations': 0,\n",
        "            'rejected_basic': 0,\n",
        "            'rejected_semantic': 0,\n",
        "            'rejected_domain': 0,\n",
        "            'rejected_duplicate': 0,\n",
        "        }\n",
        "    \n",
        "    def create_prompt(self, question: str) -> str:\n",
        "        return f'''Buat 3 variasi pertanyaan BERBEDA struktur tentang topik YANG SAMA:\n",
        "\n",
        "Pertanyaan asli: {question}\n",
        "\n",
        "PENTING:\n",
        "- Setiap variasi HARUS berbeda struktur\n",
        "- Tetap dalam konteks PMB/admisi universitas\n",
        "- Jangan ubah maksud pertanyaan\n",
        "- Format output WAJIB:\n",
        "\n",
        "VARIATION 1: [pertanyaan]\n",
        "VARIATION 2: [pertanyaan]\n",
        "VARIATION 3: [pertanyaan]'''\n",
        "    \n",
        "    def parse_variations(self, text: str) -> List[str]:\n",
        "        variations = []\n",
        "        text = text.replace('Jawaban:', '').replace('**', '').strip()\n",
        "        pattern = r'VARIATION\\s*\\d+:\\s*([^\\n]+)'\n",
        "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "        \n",
        "        for match in matches:\n",
        "            q_text = match.strip().strip('\"\\'\\''-‚Ä¢')\n",
        "            q_text = re.sub(r'\\([^)]*\\)', '', q_text).strip()\n",
        "            q_text = ' '.join(q_text.split())\n",
        "            \n",
        "            if len(q_text) > 10:\n",
        "                variations.append(q_text)\n",
        "        \n",
        "        return variations\n",
        "    \n",
        "    @torch.inference_mode()\n",
        "    def generate_batch(self, prompts: List[str]) -> List[str]:\n",
        "        try:\n",
        "            encoded = self.tokenizer(\n",
        "                prompts,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            \n",
        "            input_ids = encoded['input_ids'].to(self.config.device)\n",
        "            attention_mask = encoded['attention_mask'].to(self.config.device)\n",
        "            \n",
        "            with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
        "                outputs = self.model.generate(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    max_new_tokens=self.config.max_new_tokens,\n",
        "                    temperature=self.config.temperature,\n",
        "                    top_p=self.config.top_p,\n",
        "                    top_k=self.config.top_k,\n",
        "                    repetition_penalty=self.config.repetition_penalty,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=self.tokenizer.pad_token_id,\n",
        "                    eos_token_id=self.tokenizer.eos_token_id,\n",
        "                    use_cache=True,\n",
        "                )\n",
        "            \n",
        "            results = self.tokenizer.batch_decode(\n",
        "                outputs,\n",
        "                skip_special_tokens=True,\n",
        "                clean_up_tokenization_spaces=True\n",
        "            )\n",
        "            \n",
        "            return results\n",
        "        except Exception as e:\n",
        "            print(f'‚ùå Generation error: {str(e)[:50]}')\n",
        "            return [''] * len(prompts)\n",
        "    \n",
        "    def extract_qa(self, entry: Dict) -> Tuple[Optional[str], Optional[str]]:\n",
        "        try:\n",
        "            if 'messages' not in entry:\n",
        "                return None, None\n",
        "            q, a = None, None\n",
        "            for msg in entry['messages']:\n",
        "                if msg.get('role') == 'user':\n",
        "                    q = msg.get('content')\n",
        "                elif msg.get('role') == 'assistant':\n",
        "                    a = msg.get('content')\n",
        "            return q, a\n",
        "        except:\n",
        "            return None, None\n",
        "    \n",
        "    def create_output_entry(self, variation: str, answer: str,\n",
        "                           orig_id: str, var_num: int, metadata: Dict = None) -> Dict:\n",
        "        base_metadata = {\n",
        "            'id': f'{orig_id}_var{var_num}',\n",
        "            'format_type': 'variation',\n",
        "            'category': 'augmented',\n",
        "            'verified': True,\n",
        "            'source_id': orig_id\n",
        "        }\n",
        "        if metadata:\n",
        "            base_metadata.update(metadata)\n",
        "        \n",
        "        return {\n",
        "            'messages': [\n",
        "                {'role': 'user', 'content': variation},\n",
        "                {'role': 'assistant', 'content': answer}\n",
        "            ],\n",
        "            'metadata': base_metadata\n",
        "        }\n",
        "    \n",
        "    def process_batch_items(self, items: List[Dict]) -> List[Dict]:\n",
        "        entries = []\n",
        "        prompts = []\n",
        "        mapping = []\n",
        "        \n",
        "        for item in items:\n",
        "            q, a = self.extract_qa(item)\n",
        "            if q and a:\n",
        "                prompt = self.create_prompt(q)\n",
        "                prompts.append(prompt)\n",
        "                mapping.append((item, q, a))\n",
        "        \n",
        "        if not prompts:\n",
        "            return entries\n",
        "        \n",
        "        outputs = self.generate_batch(prompts)\n",
        "        \n",
        "        for (item, original_q, answer), output in zip(mapping, outputs):\n",
        "            try:\n",
        "                variations = self.parse_variations(output)\n",
        "                if not variations:\n",
        "                    self.stats['failed'] += 1\n",
        "                    continue\n",
        "                \n",
        "                self.dedup_val.reset()\n",
        "                valid_entries = []\n",
        "                \n",
        "                for variation in variations:\n",
        "                    # 1. BASIC VALIDATION\n",
        "                    basic_valid, basic_issues = self.quality_val.validate_question(\n",
        "                        variation, original_q\n",
        "                    )\n",
        "                    if not basic_valid:\n",
        "                        self.stats['rejected_basic'] += 1\n",
        "                        continue\n",
        "                    \n",
        "                    # 2. SEMANTIC VALIDATION\n",
        "                    semantic_valid, semantic_score = self.semantic_val.check_meaning_preservation(\n",
        "                        original_q, variation,\n",
        "                        self.semantic_val.get_embeddings([original_q])[0]\n",
        "                    )\n",
        "                    if not semantic_valid:\n",
        "                        self.stats['rejected_semantic'] += 1\n",
        "                        continue\n",
        "                    \n",
        "                    # 3. DOMAIN VALIDATION\n",
        "                    domain_valid, lost_entities = self.pmb_val.validate_entity_preservation(\n",
        "                        original_q, variation\n",
        "                    )\n",
        "                    if not domain_valid:\n",
        "                        self.stats['rejected_domain'] += 1\n",
        "                        continue\n",
        "                    \n",
        "                    # 4. DEDUPLICATION\n",
        "                    is_duplicate, dup_score = self.dedup_val.is_duplicate(\n",
        "                        variation,\n",
        "                        threshold=self.config.duplicate_threshold\n",
        "                    )\n",
        "                    if is_duplicate:\n",
        "                        self.stats['rejected_duplicate'] += 1\n",
        "                        continue\n",
        "                    \n",
        "                    valid_entries.append((variation, semantic_score))\n",
        "                \n",
        "                if not valid_entries:\n",
        "                    self.stats['failed'] += 1\n",
        "                    continue\n",
        "                \n",
        "                orig_id = item.get('metadata', {}).get('id', f'entry_{self.stats[\"total_processed\"]}')\n",
        "                \n",
        "                for var_num, (variation, sem_score) in enumerate(valid_entries, 1):\n",
        "                    entry = self.create_output_entry(\n",
        "                        variation, answer, orig_id, var_num,\n",
        "                        metadata={'semantic_score': float(sem_score)}\n",
        "                    )\n",
        "                    entries.append(entry)\n",
        "                \n",
        "                self.stats['success'] += 1\n",
        "                self.stats['total_variations'] += len(valid_entries)\n",
        "            \n",
        "            except Exception as e:\n",
        "                self.stats['failed'] += 1\n",
        "        \n",
        "        return entries\n",
        "    \n",
        "    def process_dataset(self, data: List[Dict],\n",
        "                       output_file: str = 'pmb_augmented_validated.jsonl'):\n",
        "        print(f'\\n{\\'=\\'*80}')\n",
        "        print(f'üöÄ ENHANCED AUGMENTATION WITH VALIDATION')\n",
        "        print(f'{\\'=\\'*80}')\n",
        "        print(f' Batch size: {self.config.batch_size}')\n",
        "        print(f' Total items: {len(data)}')\n",
        "        print(f' Validators: Basic + Semantic + Domain + Dedup\\n')\n",
        "        \n",
        "        start_time = time.time()\n",
        "        num_batches = (len(data) + self.config.batch_size - 1) // self.config.batch_size\n",
        "        \n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            for batch_idx in tqdm(range(num_batches), desc='Processing', unit='batch'):\n",
        "                batch_start = batch_idx * self.config.batch_size\n",
        "                batch_end = min(batch_start + self.config.batch_size, len(data))\n",
        "                batch = data[batch_start:batch_end]\n",
        "                \n",
        "                entries = self.process_batch_items(batch)\n",
        "                self.stats['total_processed'] += len(batch)\n",
        "                \n",
        "                for entry in entries:\n",
        "                    f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
        "                \n",
        "                if batch_idx % self.config.save_every_n_batches == 0:\n",
        "                    torch.cuda.empty_cache()\n",
        "                    if self.config.log_memory:\n",
        "                        self.gpu_monitor.log(f'Batch {batch_idx}/{num_batches}')\n",
        "        \n",
        "        elapsed = time.time() - start_time\n",
        "        \n",
        "        print(f'\\n{\\'=\\'*80}')\n",
        "        print(f'‚úÖ AUGMENTATION RESULTS')\n",
        "        print(f'{\\'=\\'*80}')\n",
        "        print(f' Total processed: {self.stats[\"total_processed\"]}')\n",
        "        print(f' ‚úÖ Success: {self.stats[\"success\"]}')\n",
        "        print(f' ‚ùå Failed: {self.stats[\"failed\"]}')\n",
        "        print(f' üö´ Rejected (basic): {self.stats[\"rejected_basic\"]}')\n",
        "        print(f' üö´ Rejected (semantic): {self.stats[\"rejected_semantic\"]}')\n",
        "        print(f' üö´ Rejected (domain): {self.stats[\"rejected_domain\"]}')\n",
        "        print(f' üö´ Rejected (duplicate): {self.stats[\"rejected_duplicate\"]}')\n",
        "        print(f' üìä Total variations: {self.stats[\"total_variations\"]}')\n",
        "        print(f' ‚è±Ô∏è Time: {elapsed:.1f}s')\n",
        "        if elapsed > 0:\n",
        "            print(f' üèÉ Speed: {self.stats[\"total_processed\"]/elapsed:.1f} items/sec')\n",
        "        print(f' üíæ Output: {output_file}')\n",
        "        print(f' üìà Quality - Semantic: {self.semantic_val.report()}')\n",
        "        print(f' üìà Quality - Domain: {self.pmb_val.report()}')\n",
        "        print(f' üìà Quality - Dedup: {self.dedup_val.report()}')\n",
        "        print(f'{\\'=\\'*80}\\n')\n",
        "        \n",
        "        return self.stats\n",
        "\n",
        "\n",
        "pipeline = EnhancedAugmentationPipeline(\n",
        "    config, model, tokenizer, semantic_validator,\n",
        "    dedup_validator, pmb_validator, quality_validator, gpu_monitor\n",
        ")\n",
        "print('‚úÖ Enhanced Pipeline initialized')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8Ô∏è‚É£ Test Augmentation (5 items dulu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\n' + '='*80)\n",
        "print('üß™ TEST: Augmentation Pipeline (5 items)')\n",
        "print('='*80 + '\\n')\n",
        "\n",
        "test_batch = data[50:55]\n",
        "test_entries = pipeline.process_batch_items(test_batch)\n",
        "\n",
        "print(f'\\n‚úÖ Test batch completed')\n",
        "print(f' Input items: {len(test_batch)}')\n",
        "print(f' Output variations: {len(test_entries)}')\n",
        "if len(test_batch) > 0:\n",
        "    print(f' Success rate: {len(test_entries) / (len(test_batch) * 3) * 100:.1f}%\\n')\n",
        "\n",
        "# Show samples\n",
        "for i, entry in enumerate(test_entries[:3], 1):\n",
        "    q = entry['messages'][0]['content']\n",
        "    meta = entry['metadata']\n",
        "    print(f'[Sample {i}]')\n",
        "    print(f' ID: {meta[\"id\"]}')\n",
        "    print(f' Q: {q[:70]}...')\n",
        "    print(f' Words: {len(q.split())}')\n",
        "    print(f' Semantic: {meta.get(\"semantic_score\", \"N/A\")}')\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9Ô∏è‚É£ Run Full Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FULL AUGMENTATION\n",
        "stats = pipeline.process_dataset(\n",
        "    data,\n",
        "    output_file='pmb_augmented_validated.jsonl'\n",
        ")\n",
        "\n",
        "gpu_monitor.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîü Output Analysis & Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\n' + '='*80)\n",
        "print('üìä OUTPUT ANALYSIS')\n",
        "print('='*80)\n",
        "\n",
        "with open('pmb_augmented_validated.jsonl') as f:\n",
        "    output_data = [json.loads(line) for line in f if line.strip()]\n",
        "\n",
        "print(f'\\nTotal entries: {len(output_data)}')\n",
        "\n",
        "word_counts = []\n",
        "semantic_scores = []\n",
        "for entry in output_data:\n",
        "    q = entry['messages'][0]['content']\n",
        "    word_counts.append(len(q.split()))\n",
        "    if 'semantic_score' in entry['metadata']:\n",
        "        semantic_scores.append(entry['metadata']['semantic_score'])\n",
        "\n",
        "print(f'\\nüìù Question Statistics:')\n",
        "print(f' Words: {min(word_counts)}-{max(word_counts)} (avg: {sum(word_counts)/len(word_counts):.1f})')\n",
        "print(f' All end with ?: {all(entry[\"messages\"][0][\"content\"].strip().endswith(\"?\") for entry in output_data)}')\n",
        "\n",
        "if semantic_scores:\n",
        "    print(f'\\nüß† Semantic Quality:')\n",
        "    print(f' Mean: {np.mean(semantic_scores):.3f}')\n",
        "    print(f' Min: {np.min(semantic_scores):.3f}')\n",
        "    print(f' Max: {np.max(semantic_scores):.3f}')\n",
        "\n",
        "print(f'\\nüìà Dataset Growth:')\n",
        "print(f' Original: {len(data)} entries')\n",
        "print(f' Augmented: {len(output_data)} variations')\n",
        "print(f' Ratio: {len(output_data)/len(data):.2f}x')\n",
        "\n",
        "# Export CSV\n",
        "print('\\n' + '='*80)\n",
        "print('üíæ EXPORTING FORMATS')\n",
        "print('='*80)\n",
        "\n",
        "qa_data = [\n",
        "    {\n",
        "        'question': entry['messages'][0]['content'],\n",
        "        'answer': entry['messages'][1]['content'],\n",
        "        'semantic_score': entry['metadata'].get('semantic_score', ''),\n",
        "        'verified': entry['metadata'].get('verified', '')\n",
        "    }\n",
        "    for entry in output_data\n",
        "]\n",
        "df = pd.DataFrame(qa_data)\n",
        "df.to_csv('pmb_augmented_validated.csv', index=False, encoding='utf-8')\n",
        "\n",
        "summary = {\n",
        "    'original_entries': len(data),\n",
        "    'augmented_entries': len(output_data),\n",
        "    'augmentation_ratio': len(output_data) / len(data),\n",
        "    'semantic_mean': float(np.mean(semantic_scores)) if semantic_scores else 0,\n",
        "    'all_verified': all(e['metadata'].get('verified', False) for e in output_data)\n",
        "}\n",
        "\n",
        "with open('augmentation_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f'\\n‚úÖ Files saved:')\n",
        "print(f' 1. pmb_augmented_validated.jsonl')\n",
        "print(f' 2. pmb_augmented_validated.csv')\n",
        "print(f' 3. augmentation_summary.json')\n",
        "print('\\n' + '='*80)\n",
        "print('‚úÖ AUGMENTATION PIPELINE COMPLETE')\n",
        "print('='*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£1Ô∏è‚É£ QLoRA Training (Next Phase)\n",
        "\n",
        "Augmented dataset sudah siap! Sekarang untuk QLoRA training, gunakan file: `qlora_training_guide.md`\n",
        "\n",
        "```bash\n",
        "# Dataset sudah tersimpan di:\n",
        "pmb_augmented_validated.jsonl  # Ready untuk SFTTrainer\n",
        "\n",
        "# Training akan menggunakan:\n",
        "- Gemma-3-1B base model (4-bit)\n",
        "- LoRA rank 16, alpha 32\n",
        "- 3 epochs training\n",
        "- Full training pipeline included\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "gpuType": "A100",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}